{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPython 3.6.8\n",
      "IPython 7.7.0\n",
      "\n",
      "numpy 1.17.0\n",
      "scipy 1.3.1\n",
      "pandas 0.25.0\n",
      "matplotlib 3.1.1\n",
      "sklearn 0.21.3\n",
      "\n",
      "compiler   : GCC 8.0.1 20180414 (experimental) [trunk revision 259383\n",
      "system     : Linux\n",
      "release    : 4.9.93-boot2docker\n",
      "machine    : x86_64\n",
      "processor  : x86_64\n",
      "CPU cores  : 2\n",
      "interpreter: 64bit\n",
      "Git hash   : afe74570886a7ef77226c71b24dfd90f8955c7a2\n"
     ]
    }
   ],
   "source": [
    "###########LIBRARY CELL###########\n",
    "%load_ext watermark\n",
    "%watermark -v -m -p numpy,scipy,pandas,matplotlib,sklearn -g "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########LIBRARY CELL###########\n",
    "from __future__ import division, print_function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import datetime\n",
    "import re\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########VERSION OF COMPARATOR FOR SD<->SD and SD<->TDC\n",
    "###########LIBRARY CELL###########\n",
    "#########CONSTANTS################\n",
    "COMTYPE_SDSD, COMTYPE_SDTDC, COMTYPE_TDCSD, COMTYPE_TDCTDC = 1,2,3,4;\n",
    "ENG,DE = 1,2\n",
    "comparison_type_text = {\n",
    "    COMTYPE_SDSD:\"SimadynD <=> SimadynD\",\n",
    "    COMTYPE_SDTDC:\"SimadynD <=> TDC\",\n",
    "    COMTYPE_TDCSD:\"TDC <=> SimadynD\", #TODO, not yet supported\n",
    "    COMTYPE_TDCTDC:\"TDC <=> TDC\" #TODO, not yet supported\n",
    "    }\n",
    "simadyn_lang_text = {\n",
    "    ENG:\"English\",\n",
    "    DE:\"Deutsch\"\n",
    "    }\n",
    "COMPARATOR_AUTHOR = \"Anton Tushev\" \n",
    "COMPARATOR_VERSION = \"v.33.CMB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================\r\n",
      "Advanced comparator SimadynD and TDC, software version: v.33.CMB \r\n",
      "Author: Anton Tushev \r\n",
      "Comparison type: SimadynD <=> SimadynD\r\n",
      "Comparison source #1: /notebooks/pinda/data/eko/190725\r\n",
      "Comparison source #2: /notebooks/pinda/data/eko/191031\r\n",
      "List of SimadynD subfolders for parsing is ['sx04'] \r\n",
      "List of ignored charts is ['@SIMD'] \r\n",
      "===================================================================================\n"
     ]
    }
   ],
   "source": [
    "###########LIBRARY CELL###########\n",
    "#########CONFIGURATION################\n",
    "COMPARISON_TYPE = COMTYPE_SDSD #, COMTYPE_SDTDC\n",
    "SIMADYND_LANG = DE #ENG, DE \n",
    "\n",
    "PATH_TO_DATA = '/notebooks/pinda/data'\n",
    "###PATH #1\n",
    "#PATH_TO_STRUCG_1 = '/notebooks/pinda/data/eko/test_1'\n",
    "PATH_TO_STRUCG_1 = '/notebooks/pinda/data/eko/190725'\n",
    "#PATH_TO_STRUCG_1 = '/notebooks/pinda/data/alcanc16_07_2019' \n",
    "PATH_TO_TDC_DATA_1 = 'TX01_TX01_PS.csv'\n",
    "###PATH #2\n",
    "#PATH_TO_STRUCG_2 = '/notebooks/pinda/data/eko/test_2'\n",
    "PATH_TO_STRUCG_2 = '/notebooks/pinda/data/eko/191031'\n",
    "PATH_TO_TDC_DATA_2 = 'TX01_TX01_PS.csv'\n",
    "\n",
    "PATH_TO_REPORTS = '/notebooks/pinda/reports'\n",
    "FILE_1_NAME_TEMP1 = '@source1_progress_bkup_1.csv'\n",
    "FILE_1_NAME_TEMP2 = '@source1_progress_bkup_2.csv'\n",
    "FILE_1_NAME_TEMP3 = '@source1_progress_bkup_3.csv'\n",
    "FILE_2_NAME_TEMP1 = '@source2_progress_bkup_1.csv'\n",
    "FILE_2_NAME_TEMP2 = '@source2_progress_bkup_2.csv'\n",
    "FILE_2_NAME_TEMP3 = '@source2_progress_bkup_3.csv'\n",
    "\n",
    "#SD parsing parameters\n",
    "SD_RACK_NAMES = ['sx04']\n",
    "#SD_RACK_NAMES = ['tx01']\n",
    "IGNORE_CHART = ['@SIMD']\n",
    "\n",
    "#report parameters\n",
    "#this comparator works with following data types and connectors:\n",
    "filt_type_base = ['B1','TF','N2','N4','V2','O2','V4','NF','V1','I2','TR','RR']  #,'NS'\n",
    "filt_inout = ['<']#,'-','=', '>'\n",
    "\n",
    "###TODO: outputs for sd-tdc comparison doesn't work.\n",
    "\n",
    "COMPARATOR_README = \"===================================================================================\\\n",
    "\\r\\nAdvanced comparator SimadynD and TDC, software version: %s \\r\\nAuthor: %s \\\n",
    "\\r\\nComparison type: %s\\\n",
    "\\r\\nComparison source #1: %s\\\n",
    "\\r\\nComparison source #2: %s\\\n",
    "\\r\\nList of SimadynD subfolders for parsing is %s \\\n",
    "\\r\\nList of ignored charts is %s \\\n",
    "\\r\\n===================================================================================\\\n",
    "\"%(\n",
    "    COMPARATOR_VERSION,\n",
    "    COMPARATOR_AUTHOR,\n",
    "    comparison_type_text[COMPARISON_TYPE],\n",
    "    PATH_TO_STRUCG_1 if (COMPARISON_TYPE==COMTYPE_SDSD)|(COMPARISON_TYPE==COMTYPE_SDTDC) else PATH_TO_TDC_DATA_1,\n",
    "    PATH_TO_STRUCG_2 if (COMPARISON_TYPE==COMTYPE_SDSD)|(COMPARISON_TYPE==COMTYPE_TDCSD) else PATH_TO_TDC_DATA_2,\n",
    "    SD_RACK_NAMES,\n",
    "    IGNORE_CHART)\n",
    "print(COMPARATOR_README)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########LIBRARY CELL###########\n",
    "#Generate Simadyn chart structure\n",
    "def sd_get_chart_list(path_to_struc_g,sd_rack_names):\n",
    "    #filter chart files\n",
    "    n = re.compile(\"^(((?!old).)*).((cfp)|(ofp))$\")\n",
    "    file_list=[] \n",
    "    file_list = {rack_name:[item for item in os.listdir(os.path.join(path_to_struc_g,rack_name)) \\\n",
    "        # chart file names filter conditions\n",
    "        if os.path.isfile(os.path.join(path_to_struc_g,rack_name, item)) and (n.search(item)) and len(item)<11 \\\n",
    "        ] for rack_name in sd_rack_names}\n",
    "    file_list_filtered = dict(file_list)\n",
    "    for i in file_list:\n",
    "        for k in file_list[i]:\n",
    "            chart_name = k.split('.')[0]\n",
    "            if \"%s.cfp\"%chart_name in file_list[i]:\n",
    "                try:\n",
    "                    file_list_filtered[i].remove(\"%s.ofp\"%chart_name)\n",
    "                except ValueError:\n",
    "                    pass  # do nothing\n",
    "    out_dict = dict()\n",
    "    for i in file_list_filtered:\n",
    "        out_dict[i] = dict()\n",
    "        for k in file_list_filtered[i]:\n",
    "            out_dict[i][k.split('.')[0]]=os.path.join(path_to_struc_g,i,k)\n",
    "    if out_dict:\n",
    "        return out_dict\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "#Generate Simadyn cpu mapping structure\n",
    "def sd_get_cpu_charts_list(path_to_struc_g,sd_rack_names):\n",
    "    #filter chart files\n",
    "    n = re.compile(\"^((\\d+).*)\\.(mpn)$\")\n",
    "    file_list=[] \n",
    "    file_list=[] \n",
    "    file_list = {rack_name:[item for item in os.listdir(os.path.join(path_to_struc_g,rack_name)) \\\n",
    "        # chart file names filter conditions\n",
    "        if os.path.isfile(os.path.join(path_to_struc_g,rack_name, item)) and (n.search(item)) and len(item)<11 \\\n",
    "        ] for rack_name in sd_rack_names}\n",
    "    out_dict = dict()\n",
    "    for i in file_list:\n",
    "        out_dict[i] = dict()\n",
    "        for k in file_list[i]:\n",
    "            out_dict[i][k.split('.')[0]]=os.path.join(path_to_struc_g,i,k)\n",
    "    if out_dict:\n",
    "        return out_dict\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def parse_tdc_source_file(path_to_data,filename):\n",
    "    data_tdc = pd.read_csv(os.path.join(path_to_data,filename),keep_default_na=False,na_values=[''], encoding=\"cp1251\",sep=';',header=0)\n",
    "    data_tdc = data_tdc[data_tdc.columns[[0,1,2,3,5,7,14,15]]]\n",
    "    data_tdc.columns = ['rack_cpu','chart','block','signal','value','link','type','out']\n",
    "    data_tdc = pd.concat([data_tdc, data_tdc['rack_cpu'].str.split(\"\\\\\",expand = True)[[0,1]]], axis=1) \n",
    "    data_tdc.columns = ['rack_cpu','chart','block','signal','value','link','type','out','rack','cpu']\n",
    "    data_tdc[\"out\"]= data_tdc[\"out\"].replace((\"IN\",\"OUT\"),(0,1)).astype('bool') \n",
    "    data_tdc = data_tdc[~data_tdc['chart'].isin(IGNORE_CHART)]\n",
    "    data_tdc[\"block\"] = data_tdc[\"block\"].astype(str)\n",
    "    data_tdc[\"chartblock\"] = data_tdc[\"chart\"].str.upper()+\"/\"+data_tdc[\"block\"].str.upper()\n",
    "    data_tdc[\"chart_block_signal\"] = data_tdc[\"chart\"].str.upper()+\"/\"+data_tdc[\"block\"].str.upper()+\".\"+data_tdc[\"signal\"].str.upper()\n",
    "    data_tdc['value_linked'] = np.where(data_tdc['link'].isnull(), data_tdc['value'],data_tdc['link'].str.strip('\"').str.split('\"',n=1,expand=True)[0] )\n",
    "    return data_tdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########LIBRARY CELL###########\n",
    "#########################SimadynD row data generating library#################\n",
    "NAME,TYPE_BASE,INOUT,TYPE_CONV,VALUE,CPU,CPU_SOURCE,CPU_LINK,CHART,BLOCK,LINKS  = 0,1,2,3,4,5,6,7,8,9,10\n",
    "\n",
    "#open chart for reading\n",
    "def sd_open_chart(chart_addr):\n",
    "    try:\n",
    "        with open(chart_addr) as f:\n",
    "            lines = [line.rstrip('\\n') for line in f]\n",
    "    except EnvironmentError: # parent of IOError, OSError *and* WindowsError where available\n",
    "        #print('ERROR File not exists!')\n",
    "        return -1\n",
    "    return lines\n",
    "\n",
    "#generate chart mapping: line {block name:[block line begin, block line end]}\n",
    "def sd_get_chart_map(inp_line):\n",
    "    b = re.compile(r\"^\\s*\\d+ ?([a-zA-Z0-9_]+) +: [a-zA-Z0-9_.@]+\\s*,\\s*POS=\")\n",
    "    el = re.compile(r'^ *\\d+\\s+[+]')\n",
    "    #cp = re.compile(r'\\(([a-zA-Z0-9_-]+).([a-zA-Z0-9_-]+)\\s+\\S+\\s+\\S+\\)') #not used, because not all charts consist\n",
    "    block_list = []\n",
    "    mark_bloc = False\n",
    "    #mark_cpu = False\n",
    "    out_dict = dict()\n",
    "    line_counter = 0\n",
    "    for k in inp_line:\n",
    "        #Find rack name and cpu name, e.g. MP-TX04 PN-D05_P2\n",
    "        #if not mark_cpu:\n",
    "        #    cp_s = cp.search(k)\n",
    "        #    if cp_s :\n",
    "        #        rack,cpu = cp_s.group(1),cp_s.group(2)\n",
    "        #        mark_cpu = True\n",
    "        b_s = b.search(k)\n",
    "        if b_s :\n",
    "            mark_bloc = True\n",
    "            curr_block_name = b_s.group(1)\n",
    "            block_list.append(curr_block_name)\n",
    "            mark_bloc_line_beg = line_counter\n",
    "        elif el.search(k):\n",
    "            if mark_bloc:\n",
    "                mark_bloc = False\n",
    "                out_dict[curr_block_name] = [mark_bloc_line_beg,line_counter]\n",
    "        line_counter = line_counter+1        \n",
    "    #print(len(block_list))\n",
    "    if(out_dict):\n",
    "        return out_dict\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "#check if line is signal or link\n",
    "def sd_check_line_signal(line):\n",
    "    sf = re.compile(r\"^\\s*\\d+\\s+(\\w+)\\s+(\\w+)\\s+([<>=-])\")\n",
    "    sa = re.compile(r\"^\\s*&+\\s+(\\(.*\\))\\s*$\")\n",
    "    if sf.search(line):\n",
    "        out = 1 #signal\n",
    "    elif sa.search(line):\n",
    "        out = 2 #link\n",
    "    else:\n",
    "        out = 0 #nothing\n",
    "    return out\n",
    "\n",
    "#extract data from signal line\n",
    "def sd_get_line_data(line): \n",
    "    m = re.compile(r\"\"\"\n",
    "        ^\\s*\\d+\\s+ #string number\n",
    "        (\\w+)\\s+   #signal name \n",
    "        (\\w+)\\s+   #signal type \n",
    "        ([<>=-])\\s*  #signal delimiter \n",
    "        (@TYP=(..),?)? #singal type\n",
    "        (   #value\n",
    "        ([a-zA-Z0-9_.]+)|  #can be connection to other block\n",
    "        ([$%!\\[\\]/()a-zA-Z0-9\\s_.*#\\-\\+]+)|  #can be value\n",
    "        ([$a-zA-Z0-9_]+\\s*[$a-zA-Z0-9_]*)|  #can be virtual connection\n",
    "        ([#0-9.]+\\s*\\[.*\\])|  #can be time with spaces\n",
    "        ([0-9B]+\\s+[0-9]+)|  #can be hex in bit representation\n",
    "        ('.*')| #can be text\n",
    "        (\\*[a-zA-Z0-9_.]+)  #can hardware connection: *ADRCO1\n",
    "        )? #end of value\n",
    "        ([\\s]*,)? #divider\n",
    "        (\\![a-zA-Z0-9_\\.]+)?\n",
    "        ([\\s]*,)?\n",
    "        ('.*')? #link comment\n",
    "        (,.*)? #some comment\n",
    "        (\\s*\".*\")? #signal comment\n",
    "        (&)? #check next string\n",
    "        (\\s)*\n",
    "        ($) #end of string\n",
    "        \"\"\", re.X)\n",
    "    #print(line)\n",
    "    n = m.search(line)\n",
    "    try:\n",
    "        #Check if value is XXXX but behind is telegram connection like !XXX.YY\n",
    "        if n.group(15):\n",
    "            check_val = n.group(15)\n",
    "        else:\n",
    "            check_val = n.group(6)\n",
    "        out = list([n.group(1),n.group(2),n.group(3),n.group(5),check_val])\n",
    "    except:\n",
    "        #print line for debugging\n",
    "        print(line) \n",
    "        raise\n",
    "        \n",
    "    #     #name      type       dir        type_conv  value \n",
    "    #list([n.group(1),n.group(2),n.group(3),n.group(5),n.group(6)])\n",
    "    return out\n",
    "\n",
    "#exstract link from like\n",
    "def sd_get_line_link(line): \n",
    "    m = re.compile(r\"^\\s*&+\\s+\\((.*)\\)\\s*$\")\n",
    "    n = m.search(line)     \n",
    "    return n.group(1)  \n",
    "\n",
    "#get block data: signals, values, links, data types...\n",
    "def sd_get_block_data(lines,chart,block,data_link_map_df):\n",
    "    mark_signal_row_name = None\n",
    "    mark_signal_row_data = list([[],[]]) #[[signal data],[links]]\n",
    "    mark_block_row_data = []\n",
    "    count = 0;\n",
    "    #get CPU name\n",
    "    cpu = sd_get_chart_cpu(data_link_map_df,chart)\n",
    "    #If no cpu found, it means chart is not used in SD software:\n",
    "    if cpu == -1:\n",
    "        return -1\n",
    "    for l in lines:\n",
    "        line_check_result = sd_check_line_signal(l)\n",
    "        if line_check_result:\n",
    "            if line_check_result==1:  \n",
    "                #if found a signal line\n",
    "                count = count+1\n",
    "                #add previous signal to preparation table\n",
    "                if mark_signal_row_name:\n",
    "                    #print(\"Old name:\",mark_signal_row_name)\n",
    "                    mark_block_row_data.append(mark_signal_row_data)\n",
    "                    mark_signal_row_data = list([[],[]]) #reset buffer table\n",
    "                #NAME,TYPE_BASE,INOUT,TYPE_CONV,VALUE\n",
    "                mark_signal_row_data[0] = sd_get_line_data(l)\n",
    "                mark_signal_row_data[0].append(cpu)\n",
    "                #If value is $LINK here is a logic:\n",
    "                #Copy value $LINK to link column\n",
    "                if re.search(r'^\\$',str(mark_signal_row_data[0][VALUE])) and mark_signal_row_data[0][INOUT]==\"<\":\n",
    "                    link = mark_signal_row_data[0][VALUE].split()[0] #CONVERT '$LINK PN' => '$LINK'\n",
    "                    mark_signal_row_data[1].append(link)\n",
    "                    #get CPU_SOURCE\n",
    "                    cpu_source = sd_get_block_link_source_cpu(data_link_map_df,link,chart)\n",
    "                    mark_signal_row_data[0].append(cpu_source)\n",
    "                    #get CPU_LINK   (means CPU_SOURCE_LINK)\n",
    "                    mark_signal_row_data[0].append(cpu_source+\"_\"+link)\n",
    "                else:\n",
    "                    #get CPU_SOURCE\n",
    "                    mark_signal_row_data[0].append(cpu)\n",
    "                    #get CPU_LINK   (means CPU_SOURCE_LINK)\n",
    "                    mark_signal_row_data[0].append(None)\n",
    "                    \n",
    "                mark_signal_row_name = mark_signal_row_data[0][NAME]\n",
    "                #get CHART,BLOCK\n",
    "                mark_signal_row_data[0].append(chart)\n",
    "                mark_signal_row_data[0].append(block)\n",
    "            elif line_check_result==2:\n",
    "                #if found a link line: (SOMETHING,SOMETHING2)\n",
    "                mark_signal_row_data[1].extend(sd_get_line_link(l).split(','))\n",
    "            #print(mark_signal_row_data)            \n",
    "        else:\n",
    "            pass #skip string\n",
    "    #Add last signal to preparation storage\n",
    "    mark_block_row_data.append(mark_signal_row_data)\n",
    "    return mark_block_row_data\n",
    "\n",
    "#convert list to pandas data frame\n",
    "def sd_conv_block_data_to_df(block_data):\n",
    "    #list element of row signal data:[[NAME,TYPE_BASE,INOUT,TYPE_CONV,VALUE],[links]]\n",
    "    block_df = pd.DataFrame([[j for j in k[0]]+[k[1]] for k in block_data])\n",
    "    return block_df\n",
    "#convert list to better representation\n",
    "def sd_conv_block_data_to_list(block_data):\n",
    "    #list element of row signal data:[[NAME,TYPE_BASE,INOUT,TYPE_CONV,VALUE,CPU,CPU_SOURCE,CPU_LINK,CHART,BLOCK],[links]]\n",
    "    block_data_list = [[j for j in k[0]]+[k[1]] for k in block_data]\n",
    "    return block_data_list\n",
    "\n",
    "\n",
    "#create df row with link source structure: chart_block_signal,cpu_link\n",
    "def sd_get_source_link(block_data_list):\n",
    "    #block_data_list: (NAME,TYPE_BASE,INOUT,TYPE_CONV,VALUE,CPU,CPU_SOURCE,CPU_LINK,CHART,BLOCK ),LINKS\n",
    "    m = re.compile(r\"^\\$\\w+\")\n",
    "    out_df = []\n",
    "    for j in block_data_list:\n",
    "        if j[INOUT]==\">\":\n",
    "            if(j[INOUT]):\n",
    "                \n",
    "                val = str(j[VALUE])\n",
    "                n = m.search(val) \n",
    "                if n:\n",
    "                    #chart\\block.signal,cpu_link\n",
    "                    chart_block_signal = j[CHART].upper()+'\\\\'+j[BLOCK].upper()+'.'+j[NAME].upper()\n",
    "                    cpu_link = j[CPU]+'_'+val.split()[0] #convert '$LINK PN' => '$LINK'\n",
    "                    #print(chart_block_signal,\"=>\",cpu_link)\n",
    "                    out_df.append([chart_block_signal,cpu_link])\n",
    "            #look for source link in link array\n",
    "            for k in j[LINKS]:\n",
    "                if(k):\n",
    "                    n = m.search(str(k)) \n",
    "                    if n:\n",
    "                        #chart\\block.signal,cpu_link\n",
    "                        chart_block_signal = j[CHART].upper()+'\\\\'+j[BLOCK].upper()+'.'+j[NAME].upper()\n",
    "                        cpu_link = j[CPU]+'_'+str(k).split()[0] #convert '$LINK PN' => '$LINK'\n",
    "                        #print(chart_block_signal,\"=>\",cpu_link)\n",
    "                        out_df.append([chart_block_signal,cpu_link])\n",
    "    return out_df\n",
    "\n",
    "#generate data of all charts and generate data of link sources\n",
    "def sd_build_data_s1(sd_chart_list,data_link_map_df):\n",
    "    \n",
    "    data_list = []\n",
    "    data_link_source_list = []\n",
    "    for i in sd_chart_list:\n",
    "        for (k,v) in sd_chart_list[i].items(): \n",
    "            chart = k.upper()\n",
    "            chart_lines = sd_open_chart(v)\n",
    "            chart_mapping = sd_get_chart_map(chart_lines)\n",
    "            if chart_mapping==-1:\n",
    "                print(\"Parsing '%s' chart with 0 functions\"%(chart))\n",
    "                continue\n",
    "            else:\n",
    "                print(\"Parsing '%s' chart with %s functions\"%(chart,len(chart_mapping)))\n",
    "            \n",
    "            for block_map in chart_mapping:\n",
    "                #generating array 'signal','type_base','inout','type_conv','value','link','chart','block','cpu'\n",
    "                block_lines = chart_lines[chart_mapping[block_map][0]:chart_mapping[block_map][1]]\n",
    "                #get block data: NAME,TYPE_BASE,INOUT,TYPE_CONV,VALUE,CPU,CPU_SOURCE,CPU_LINK,CHART,BLOCK\n",
    "                block_data = sd_get_block_data(block_lines,chart,block_map,data_link_map_df)\n",
    "                if block_data == -1:\n",
    "                    print(\"Parsing cancelled: '%s' chart is not used in SimadynD program\"%(chart))\n",
    "                    break\n",
    "                #print(\"before \",block_data)\n",
    "                #print(\"-- \")\n",
    "                block_data_list = sd_conv_block_data_to_list(block_data)\n",
    "                #print(\"after \",block_data_list)\n",
    "                #print(\"-- \")\n",
    "                #Add block data to accumulation list\n",
    "                data_list.extend(block_data_list)\n",
    "                #create link source array\n",
    "                block_link_source_list = sd_get_source_link(block_data_list)\n",
    "                if len(block_link_source_list)!=0:\n",
    "                    #print(block_link_source_list)\n",
    "                    data_link_source_list.extend(block_link_source_list) \n",
    "    #Convert to DataFrame for easy work\n",
    "    data_link_source_df = pd.DataFrame(data_link_source_list, columns=['chart_block_signal','cpu_link'])\n",
    "    data_df = pd.DataFrame(data_list, columns=['signal','type_base','inout','type_conv','value','cpu',\\\n",
    "                                                         'cpu_source','cpu_link','chart','block','link'])\n",
    "    data_df[\"block\"] = data_df[\"block\"].astype(str)\n",
    "    return data_df,data_link_source_df\n",
    "\n",
    "#parsing mapping file\n",
    "#generate cpu,link db by cpu mapping file\n",
    "def sd_build_cpu_map(sd_chart_list):\n",
    "    data_link_map_list = []\n",
    "    bg_lang_options = {ENG:\"Connector list of the function packets\",\n",
    "                       DE:\"Konnektorliste der Funktionspakete\"}\n",
    "    cp = re.compile(r'^\\s+\\d+\\s+([a-zA-Z0-9_-]+)$') #find CPU name in file\n",
    "    bg = re.compile(r'^\\s+\\d+\\s+%s'%(bg_lang_options[SIMADYND_LANG])) # find beginning of link information\n",
    "    en = re.compile(r'^\\s*\\d+\\s+\\*')\n",
    "    ch = re.compile(r'^\\s*\\d+\\s+FP-([a-zA-Z0-9_\\-@]+)')\n",
    "    lk = re.compile(r'^\\s*\\d+\\s+(\\$[a-zA-Z0-9_\\-@]+)\\s+([\\w]+)\\s+((<)|(>))\\s+([\\w]+)\\s+([a-zA-Z0-9_-]+)$')\n",
    "    \n",
    "    for i in sd_chart_list:\n",
    "        for (k,v) in sd_chart_list[i].items():\n",
    "            chart_lines = sd_open_chart(v)\n",
    "            print(\"Parsing cpu file '%s'  \"%(k))\n",
    "            cpu_mark = False\n",
    "            beg_mark = False\n",
    "            end_mark = False\n",
    "            for l in chart_lines:\n",
    "                ##Find CPU name\n",
    "                if not cpu_mark:\n",
    "                    cp_s = cp.search(l)\n",
    "                    if cp_s :\n",
    "                        cpu = cp_s.group(1)\n",
    "                        cpu_mark = True\n",
    "                        print(cpu)   \n",
    "                else:\n",
    "                    if not beg_mark:\n",
    "                        if bg.search(l):\n",
    "                            beg_mark = True\n",
    "                    else:\n",
    "                        #check end\n",
    "                        if en.search(l):\n",
    "                            end_mark = True\n",
    "                            break\n",
    "                        else:\n",
    "                            #main block:\n",
    "                            ch_s = ch.search(l)\n",
    "                            if ch.search(l): #if chart beginning 1739\tFP-MONIW1\n",
    "                                chart = ch_s.group(1)\n",
    "                            else:\n",
    "                                lk_s = lk.search(l)\n",
    "                                if lk_s: #if chart link definition 1744\t   $DMALB1 T5 < PN     PN-D01_P1\n",
    "                                    #'cpu','chart','out','link','cpu_source'\n",
    "                                    data_link_map_list.append([cpu, chart.upper(),lk_s.group(3),lk_s.group(1),lk_s.group(7)])\n",
    "    data_link_map_df = pd.DataFrame(data_link_map_list, columns=['cpu','chart','out','link','cpu_source'])\n",
    "    return data_link_map_df\n",
    "\n",
    "def sd_get_chart_cpu(data_link_map_df,chart):\n",
    "    try:\n",
    "        result = data_link_map_df[data_link_map_df['chart']==chart].iloc[0]['cpu']\n",
    "    except IndexError:\n",
    "        result = -1\n",
    "    return result\n",
    "def sd_get_block_link_source_cpu(data_link_map_df,link,chart):\n",
    "    #print(\"debug:\",link,chart)\n",
    "    return data_link_map_df[(data_link_map_df['chart']==chart)&\\\n",
    "                            (data_link_map_df['link']==link)&\\\n",
    "                            (data_link_map_df['out']==\"<\")].iloc[0]['cpu_source']\n",
    "\n",
    "def sd_get_link_partner(data_link_source_df,cpu_link):\n",
    "    try:\n",
    "        return data_link_source_df[(data_link_source_df['cpu_link']==cpu_link)].iloc[0]['chart_block_signal']\n",
    "    except IndexError:\n",
    "        return None\n",
    "    \n",
    "###########LIBRARY CELL###########\n",
    "#find partner link and put in 'value_linked' column.\n",
    "#This column should be used for comparison\n",
    "def sd_rect_data_s1(data_df,data_link_source_df):\n",
    "    def lookup(x):\n",
    "        partner = sd_get_link_partner(data_link_source_df,x)\n",
    "        return partner\n",
    "    data_df['value_linked'] = data_df['cpu_link'].apply(lookup)\n",
    "    return data_df\n",
    "\n",
    "#copy values in 'value_linked' - to get proper view: value or link to other block. \n",
    "#This column should be used for comparison\n",
    "#Also convert in-chart link to absolute representation e.g. ENMESS.Q => MONIW1\\ENMESS.Q\n",
    "def sd_rect_data_s2(data_df):\n",
    "    def lookup(x):\n",
    "        if x['value_linked'] is None:\n",
    "            if re.search(r'^[a-zA-Z0-9_\\-@]+\\.[a-zA-Z0-9_\\-@]+$',str(x['value'])) and not re.search(r'^[0-9]+\\.[0-9]+$',str(x['value'])):\n",
    "                if not x['type_base'] in ['CR','NK','TR','RR']:\n",
    "                    return x['chart']+\"\\\\\"+x['value']\n",
    "                else:\n",
    "                    return x['value']\n",
    "            else:\n",
    "                #convert output 'link' array to string and return to 'value_linked'\n",
    "                #TODO: doesn't work with $links -> should be extended for this case.\n",
    "                if x['value'] is None and x['link']:\n",
    "                    return ','.join(x['link'])\n",
    "                else:\n",
    "                    return x['value']  \n",
    "        else:\n",
    "            return x['value_linked']\n",
    "    data_df['value_linked'] = data_df.apply(lookup,axis = 1)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########EXECUTION CELL###########\n",
    "if(COMPARISON_TYPE==COMTYPE_SDSD):\n",
    "    sd1_chart_list = sd_get_chart_list(PATH_TO_STRUCG_1,SD_RACK_NAMES) \n",
    "    sd1_cpu_charts_list = sd_get_cpu_charts_list(PATH_TO_STRUCG_1,SD_RACK_NAMES)\n",
    "    sd2_chart_list = sd_get_chart_list(PATH_TO_STRUCG_2,SD_RACK_NAMES) \n",
    "    sd2_cpu_charts_list = sd_get_cpu_charts_list(PATH_TO_STRUCG_2,SD_RACK_NAMES) \n",
    "elif  (COMPARISON_TYPE==COMTYPE_SDTDC):  \n",
    "    sd1_chart_list = sd_get_chart_list(PATH_TO_STRUCG_1,SD_RACK_NAMES) \n",
    "    sd1_cpu_charts_list = sd_get_cpu_charts_list(PATH_TO_STRUCG_1,SD_RACK_NAMES)\n",
    "    tdc2_data_a1 = parse_tdc_source_file(PATH_TO_DATA,PATH_TO_TDC_DATA_2)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCSD):  \n",
    "    raise Exception('TDC to SD not implemented yet')\n",
    "    tdc1_data_a1 = parse_tdc_source_file(PATH_TO_DATA,PATH_TO_TDC_DATA_1)\n",
    "    sd2_chart_list = sd_get_chart_list(PATH_TO_STRUCG_2,SD_RACK_NAMES) \n",
    "    sd2_cpu_charts_list = sd_get_cpu_charts_list(PATH_TO_STRUCG_2,SD_RACK_NAMES)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCTDC):  \n",
    "    raise Exception('TDC to TDC not implemented yet')\n",
    "    tdc1_data_a1 = parse_tdc_source_file(PATH_TO_DATA,PATH_TO_TDC_DATA_1)\n",
    "    tdc2_data_a1 = parse_tdc_source_file(PATH_TO_DATA,PATH_TO_TDC_DATA_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 129\tX01 V2 < 0H0,!E2C113.0011&\n",
      "0H0 !E2C113.0011\n",
      " 325\tI1  B1 < VIHW09.Q15&\n",
      "VIHW09.Q15 None\n",
      "1397\tCCI N2 < DMVI04.Y \n",
      "DMVI04.Y None\n",
      "1447\tI3  B1 < VII250.Q9  ,'ENNROLRD' \"SECTION NOT READY FOR ROLLING\"\n",
      "VII250.Q9 None\n",
      "1229\tAD  NK = *ADRCO1,'DO5I2X6A'\n",
      "*ADRCO1 None\n",
      " 100\tIS  V2 < $VST1LS\"Limit switches stand movements 1\"\n",
      "$VST1LS None\n",
      "  81\tIS  V2 < $VISW17\n",
      "$VISW17 None\n",
      " 163\tQ1  B1 > ,'Man__CPS'            \"MAINTENANCE COIL PREP STATION\"\n",
      "None None\n",
      " 115\tI1  B1 < VIHW17.Q2&\n",
      "VIHW17.Q2 None\n",
      " 519\tX1  N2 < @TYP=V2,0HFFFF\n",
      "0HFFFF None\n",
      " 150\tY   N2 > @TYP=O2,'RP_ALL'&\n",
      "'RP_ALL' None\n",
      " 520\tX2  N2 < @TYP=V2,LB2.QS&\n",
      "LB2.QS None\n",
      " 527\tQS  V2 > $VGAU02                \"Diagnose 2nd direction\"\n",
      "$VGAU02                 None\n",
      " 528\tQ   B1 > \n",
      "None None\n",
      "  31\tAD  NK = D02_I1.X8C\n",
      "D02_I1.X8C None\n",
      "  32\tDM  B1 - 1\n",
      "1 None\n",
      "  22\tAR  NS - 'B1G1X1'\n",
      "'B1G1X1' None\n",
      "  21\tCTS CR - D0300A\n",
      "D0300A None\n",
      "1151\tT1  NS - '@F@85@ EN QFC Coil P. Shear Tab. Air Float'\n",
      "'@F@85@ EN QFC Coil P. Shear Tab. Air Float' None\n",
      "1199\tX1  N2 < 0%\n",
      "0% None\n",
      "1200\tX2  N2 < SEND07.QS\n",
      "SEND07.QS None\n",
      "1201\tI   B1 < EN_MES.Q,'EN_MES'&\n",
      "EN_MES.Q None\n",
      " 566\tLZU NF < #10000[ms],'RunTime '  \"Run time monitoring\"\n",
      "#10000[ms] None\n",
      "1207\tX2  N2 < 18.298%                \"18,3 representa 3000 kg\"\n",
      "18.298%                 None\n",
      "1283\tX2  NF < #0\n",
      "#0 None\n",
      "1304\tX2  NF < #26,'GRAUS'\n",
      "#26 None\n",
      " 905\tX2  NF < #100.0000000E3\n",
      "#100.0000000E3 None\n",
      "1074\tY   NF > @TYP=TF,$PTTI_A,'PTTI_AUT'&\n",
      "$PTTI_A None\n",
      "1073\tX   NF < @TYP=TF,10[S]\n",
      "10[S] None\n",
      "2983\tX2  NF < #530.0000000E-3,'m'\n",
      "#530.0000000E-3 None\n",
      " 439\tFBU NF < #500[ms],'Feedback'    \"Feedback monitoring\"\n",
      "#500[ms] None\n",
      " 494\tI   B1 < STG641.Q\n",
      "STG641.Q None\n",
      " 495\tY   N2 > @TYP=V2\n",
      "None None\n",
      "  17\tX1  NF < $XIHW05,SCAL=1[m]\n",
      "$XIHW05 None\n",
      " 374\tT   TF <  3           [s]\n",
      "3           [s] None\n",
      " 169\tT   TF <            2[s ]\n",
      "2[s ] None\n",
      "2946\tOR  V2 < 0B00000000 00000000\n",
      "0B00000000 00000000 None\n",
      " 108\tX05 NF < #2500.0[V/(m/s)],'ky'  \"Scaling factor for control output\"&\n",
      "#2500.0[V/(m/s)] None\n",
      "2756\tCRT TR = !EXM504\n",
      "!EXM504 None\n",
      " 337\tQ1  B1 >\n",
      "None None\n",
      "1407\tT   TF < #2.5 [s]\n",
      "#2.5 [s] None\n",
      "  87\tQ3  B1 > $E_STOP PN,'E_STOP'&\n",
      "$E_STOP PN None\n",
      " 267\tX   NF < #1E+6\n",
      "#1E+6 None\n",
      " 602\tX2  NF < #-0.200E-09[m/(N*s)]   \"Umrechnung [N] auf [m]\"\n",
      "#-0.200E-09[m/(N*s)]    None\n",
      " 108\tTD  TF < #1E-3          [S]      \"Verzoegerungszeit\"\n",
      "#1E-3          [S]       None\n"
     ]
    }
   ],
   "source": [
    "###########TEST CELL, IGNORE###########\n",
    "#n = re.search(\"^(((?!old).)*).((cfp)|(ofp))$\", 'pray1___20131208.cfp')\n",
    "test_txt = []\n",
    "test_txt.append(\" 129\tX01 V2 < 0H0,!E2C113.0011&\")\n",
    "test_txt.append(\" 325\tI1  B1 < VIHW09.Q15&\")\n",
    "test_txt.append(\"1397\tCCI N2 < DMVI04.Y \")\n",
    "test_txt.append(\"1447\tI3  B1 < VII250.Q9  ,'ENNROLRD' \\\"SECTION NOT READY FOR ROLLING\\\"\")\n",
    "test_txt.append(\"1229\tAD  NK = *ADRCO1,'DO5I2X6A'\")\n",
    "test_txt.append(\" 100\tIS  V2 < $VST1LS\\\"Limit switches stand movements 1\\\"\")\n",
    "test_txt.append('  81\tIS  V2 < $VISW17')\n",
    "test_txt.append(\" 163\tQ1  B1 > ,'Man__CPS'            \\\"MAINTENANCE COIL PREP STATION\\\"\")\n",
    "test_txt.append(' 115\tI1  B1 < VIHW17.Q2&')\n",
    "test_txt.append(' 519\tX1  N2 < @TYP=V2,0HFFFF')\n",
    "test_txt.append(\" 150\tY   N2 > @TYP=O2,'RP_ALL'&\")\n",
    "test_txt.append(' 520\tX2  N2 < @TYP=V2,LB2.QS&')\n",
    "test_txt.append(' 527\tQS  V2 > $VGAU02                \"Diagnose 2nd direction\"')\n",
    "test_txt.append(' 528\tQ   B1 > ')\n",
    "test_txt.append(\"  31\tAD  NK = D02_I1.X8C\")\n",
    "test_txt.append(\"  32\tDM  B1 - 1\")\n",
    "test_txt.append(\"  22\tAR  NS - 'B1G1X1'\")\n",
    "test_txt.append(\"  21\tCTS CR - D0300A\")\n",
    "test_txt.append(\"1151\tT1  NS - '@F@85@ EN QFC Coil P. Shear Tab. Air Float'\")\n",
    "test_txt.append(\"1199\tX1  N2 < 0%\")\n",
    "test_txt.append(\"1200\tX2  N2 < SEND07.QS\")\n",
    "test_txt.append(\"1201\tI   B1 < EN_MES.Q,'EN_MES'&\")\n",
    "test_txt.append(\" 566\tLZU NF < #10000[ms],'RunTime '  \\\"Run time monitoring\\\"\")\n",
    "test_txt.append(\"1207\tX2  N2 < 18.298%                \\\"18,3 representa 3000 kg\\\"\")\n",
    "test_txt.append(\"1283\tX2  NF < #0\")\n",
    "test_txt.append(\"1304\tX2  NF < #26,'GRAUS'\")\n",
    "test_txt.append(\" 905\tX2  NF < #100.0000000E3\")\n",
    "test_txt.append(\"1074\tY   NF > @TYP=TF,$PTTI_A,'PTTI_AUT'&\")\n",
    "test_txt.append(\"1073\tX   NF < @TYP=TF,10[S]\")\n",
    "test_txt.append(\"2983\tX2  NF < #530.0000000E-3,'m'\")\n",
    "test_txt.append(\" 439\tFBU NF < #500[ms],'Feedback'    \\\"Feedback monitoring\\\"\")\n",
    "test_txt.append(\" 494\tI   B1 < STG641.Q\")\n",
    "test_txt.append(\" 495\tY   N2 > @TYP=V2\")\n",
    "test_txt.append(\"  17\tX1  NF < $XIHW05,SCAL=1[m]\")\n",
    "test_txt.append(\" 374\tT   TF <  3           [s]\")\n",
    "test_txt.append(\" 169\tT   TF <            2[s ]\")\n",
    "test_txt.append(\"2946\tOR  V2 < 0B00000000 00000000\")\n",
    "test_txt.append(\" 108\tX05 NF < #2500.0[V/(m/s)],'ky'  \\\"Scaling factor for control output\\\"&\")\n",
    "test_txt.append(\"2756\tCRT TR = !EXM504\")\n",
    "test_txt.append(\" 337\tQ1  B1 >\")\n",
    "test_txt.append(\"1407\tT   TF < #2.5 [s]\")\n",
    "test_txt.append(\"  87\tQ3  B1 > $E_STOP PN,'E_STOP'&\")\n",
    "test_txt.append(\" 267\tX   NF < #1E+6\")\n",
    "test_txt.append(' 602\tX2  NF < #-0.200E-09[m/(N*s)]   \"Umrechnung [N] auf [m]\"')\n",
    "test_txt.append(' 108\tTD  TF < #1E-3          [S]      \"Verzoegerungszeit\"')\n",
    "test_txt2 = \" 439\tFBU NF < #500[ms],'Feedback'    \\\"Feedback monitoring\\\"\"\n",
    "\n",
    "\n",
    "#m = re.compile(r\"^\\s*\\d+\\s+(\\w+)\\s+(\\w+)\\s+([<>])\\s+(@TYP=(..),)?([^@\\\"',&]+)([,\\\"'&]|$)\", )\n",
    "m = re.compile(r\"\"\"\n",
    "        ^\\s*\\d+\\s+ #string number\n",
    "        (\\w+)\\s+   #signal name \n",
    "        (\\w+)\\s+   #signal type \n",
    "        ([<>=-])\\s*  #signal delimiter \n",
    "        (@TYP=(..),?)? #singal type\n",
    "        (   #value\n",
    "        ([a-zA-Z0-9_.]+)|  #can be connection to other block\n",
    "        ([$%!\\[\\]/()a-zA-Z0-9\\s_.*#\\-\\+]+)|  #can be value\n",
    "        ([$a-zA-Z0-9_]+\\s*[$a-zA-Z0-9_]*)|  #can be virtual connection\n",
    "        ([#0-9.]+\\s*\\[.*\\])|  #can be time with spaces\n",
    "        ([0-9B]+\\s+[0-9]+)|  #can be hex in bit representation\n",
    "        ('.*')| #can be text\n",
    "        (\\*[a-zA-Z0-9_.]+)  #can hardware connection: *ADRCO1\n",
    "        )? #end of value\n",
    "        ([\\s]*,)? #divider\n",
    "        (\\![a-zA-Z0-9_\\.]+)?\n",
    "        ([\\s]*,)?\n",
    "        ('.*')? #link comment\n",
    "        (,.*)? #some comment\n",
    "        (\\s*\".*\")? #signal comment\n",
    "        (&)? #check next string\n",
    "        (\\s)*\n",
    "        ($) #end of string\n",
    "        \"\"\", re.X)\n",
    "k = re.compile(r\"^\\s*\\d+\\s+(\\w+)\\s+(\\w+)\\s+([<>=-])\")\n",
    "\n",
    "for a in test_txt:\n",
    "    n = m.search(a)\n",
    "    print(a)\n",
    "    #print(n.group(1),n.group(2),n.group(3),n.group(5),n.group(6),n.group(13))\n",
    "    #print(n)\n",
    "    print(n.group(6),n.group(15))\n",
    "    #print(n)\n",
    "    \n",
    "#n = m.search(test_txt2)\n",
    "#print(\"OUT:\",n.group(1),n.group(2),n.group(3),n.group(6),n.group(13))\n",
    "\n",
    "#for a in test_txt:\n",
    "#    n = k.search(a)\n",
    "#    print(n)\n",
    "\n",
    "#for a in test_txt:\n",
    "#    n = k.search(a)\n",
    "#    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_sre.SRE_Match object; span=(0, 37), match=' 402\\t   $WWSTAT T5 <  FL    PN-D01_P1'>\n"
     ]
    }
   ],
   "source": [
    "###########TEST CELL, IGNORE###########\n",
    "#test_txt=\"   &\tV4.2.6 FP-COP CHK 22.02.;9 07:54  (MP-TX04.PN-D05_P2 13.09.;6 05:02)\"\n",
    "#test_txt = \"   1 PN-D01_P1\"\n",
    "#test_txt = \"1832\t*\"\n",
    "#test_txt = \"\"\n",
    "#test_txt = \"  89 Connector list of the function packets\"\n",
    "test_txt = \" 402\t   $WWSTAT T5 <  FL    PN-D01_P1\"\n",
    "#test_txt = \" 830\t   $WWSTAT T4 <  PN     PN-D01_P1\"\n",
    "#test_txt = \"1816\tFP-@SEND1,COM\"\n",
    "#test_txt = \"1679\t   $VINS02 T4 < PN     PN-D01_P1\"\n",
    "#test_txt = \" 830\t   $DIADE  T4 < PN     PN-D01_P1\"\n",
    "#test_txt = \"ENMESS.Q1231\"\n",
    "\n",
    "#cp = re.compile(r'^\\s+\\d+\\s+([a-zA-Z0-9_-]+)$')\n",
    "#cp = re.compile(r'^\\s+\\d+\\s+Connector list of the function packets')\n",
    "#cp = re.compile(r'^\\s*\\d+\\s+\\*')\n",
    "#cp = re.compile(r'^\\s*\\d+\\s+FP-([a-zA-Z0-9_\\-@]+)')\n",
    "cp = re.compile(r'^\\s*\\d+\\s+(\\$[a-zA-Z0-9_\\-@]+)\\s+([\\w]+)\\s+((<)|(>))\\s+([\\w]+)\\s+([a-zA-Z0-9_-]+)$')\n",
    "#cp = re.compile(r'^[a-zA-Z0-9_\\-@]+\\.[a-zA-Z0-9_\\-@]+$')\n",
    "cp_s = cp.search(test_txt)\n",
    "print(cp_s)\n",
    "#print(cp_s.group(1),cp_s.group(3),cp_s.group(7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing cpu file '1sx04'  \n",
      "PN-D01_P1\n",
      "Parsing cpu file '2sx04'  \n",
      "PN-D04_P2\n",
      "Parsing cpu file '3sx04'  \n",
      "PN-D06_P3\n",
      "Parsing cpu file '4sx04'  \n",
      "PN-D08_P4\n",
      "Parsing cpu file '5sx04'  \n",
      "PN-D10_P5\n",
      "Parsing 'RCEV_2' chart with 30 functions\n",
      "Parsing 'RCEV_3' chart with 30 functions\n",
      "Parsing 'RCEV_4' chart with 30 functions\n",
      "Parsing 'RCEV_5' chart with 30 functions\n",
      "Parsing 'REF__1' chart with 172 functions\n",
      "Parsing 'REF__2' chart with 172 functions\n",
      "Parsing 'REF__3' chart with 172 functions\n",
      "Parsing 'REF__4' chart with 172 functions\n",
      "Parsing 'REF__5' chart with 172 functions\n",
      "Parsing 'SEQ__1' chart with 185 functions\n",
      "Parsing 'SEQ__2' chart with 185 functions\n",
      "Parsing 'SEQ__3' chart with 185 functions\n",
      "Parsing 'SEQ__4' chart with 185 functions\n",
      "Parsing 'SEQ__5' chart with 185 functions\n",
      "Parsing 'SERV_1' chart with 12 functions\n",
      "Parsing 'SERV_2' chart with 12 functions\n",
      "Parsing 'SERV_3' chart with 12 functions\n",
      "Parsing 'SERV_4' chart with 12 functions\n",
      "Parsing 'SERV_5' chart with 12 functions\n",
      "Parsing 'TWAS_1' chart with 27 functions\n",
      "Parsing cancelled: 'TWAS_1' chart is not used in SimadynD program\n",
      "Parsing 'WBER_1' chart with 130 functions\n",
      "Parsing 'WBER_2' chart with 130 functions\n",
      "Parsing 'WBER_3' chart with 130 functions\n",
      "Parsing 'WBER_4' chart with 130 functions\n",
      "Parsing 'WBER_5' chart with 130 functions\n",
      "Parsing 'WPS__1' chart with 7 functions\n",
      "Parsing cancelled: 'WPS__1' chart is not used in SimadynD program\n",
      "Parsing 'WPS__2' chart with 7 functions\n",
      "Parsing cancelled: 'WPS__2' chart is not used in SimadynD program\n",
      "Parsing 'WPS__3' chart with 7 functions\n",
      "Parsing cancelled: 'WPS__3' chart is not used in SimadynD program\n",
      "Parsing 'WPS__4' chart with 7 functions\n",
      "Parsing cancelled: 'WPS__4' chart is not used in SimadynD program\n",
      "Parsing 'WPS__5' chart with 7 functions\n",
      "Parsing cancelled: 'WPS__5' chart is not used in SimadynD program\n",
      "Parsing 'MV6__4' chart with 122 functions\n",
      "Parsing 'MV6__5' chart with 122 functions\n",
      "Parsing 'MV7__1' chart with 261 functions\n",
      "Parsing 'MV7__2' chart with 260 functions\n",
      "Parsing 'MV7__3' chart with 260 functions\n",
      "Parsing 'MV7__4' chart with 260 functions\n",
      "Parsing 'MV7__5' chart with 260 functions\n",
      "Parsing 'MV8__1' chart with 110 functions\n",
      "Parsing 'OPD__1' chart with 95 functions\n",
      "Parsing 'OPD__2' chart with 95 functions\n",
      "Parsing 'OPD__3' chart with 95 functions\n",
      "Parsing 'OPD__4' chart with 95 functions\n",
      "Parsing 'OPD__5' chart with 95 functions\n",
      "Parsing 'OUTHW1' chart with 71 functions\n",
      "Parsing 'OUTHW2' chart with 56 functions\n",
      "Parsing 'OUTHW3' chart with 58 functions\n",
      "Parsing 'OUTHW4' chart with 60 functions\n",
      "Parsing 'OUTHW5' chart with 56 functions\n",
      "Parsing 'OUTSW1' chart with 93 functions\n",
      "Parsing 'OUTSW2' chart with 69 functions\n",
      "Parsing 'OUTSW3' chart with 63 functions\n",
      "Parsing 'OUTSW4' chart with 63 functions\n",
      "Parsing 'OUTSW5' chart with 66 functions\n",
      "Parsing 'PARA_1' chart with 9 functions\n",
      "Parsing 'PARA_2' chart with 9 functions\n",
      "Parsing 'PARA_3' chart with 9 functions\n",
      "Parsing 'PARA_4' chart with 9 functions\n",
      "Parsing 'PARA_5' chart with 9 functions\n",
      "Parsing 'RACK_1' chart with 21 functions\n",
      "Parsing 'RCEV_1' chart with 34 functions\n",
      "Parsing 'MMII_2' chart with 76 functions\n",
      "Parsing 'MMII_3' chart with 76 functions\n",
      "Parsing 'MMII_4' chart with 76 functions\n",
      "Parsing 'MMII_5' chart with 76 functions\n",
      "Parsing 'MV1__1' chart with 253 functions\n",
      "Parsing 'MV1__2' chart with 233 functions\n",
      "Parsing 'MV1__3' chart with 232 functions\n",
      "Parsing 'MV1__4' chart with 232 functions\n",
      "Parsing 'MV1__5' chart with 232 functions\n",
      "Parsing 'MV2__1' chart with 151 functions\n",
      "Parsing 'MV2__2' chart with 151 functions\n",
      "Parsing 'MV2__3' chart with 151 functions\n",
      "Parsing 'MV2__4' chart with 151 functions\n",
      "Parsing 'MV2__5' chart with 204 functions\n",
      "Parsing 'MV3__1' chart with 132 functions\n",
      "Parsing 'MV3__2' chart with 132 functions\n",
      "Parsing 'MV3__3' chart with 132 functions\n",
      "Parsing 'MV3__4' chart with 132 functions\n",
      "Parsing 'MV3__5' chart with 132 functions\n",
      "Parsing 'MV4__1' chart with 148 functions\n",
      "Parsing 'MV4__2' chart with 148 functions\n",
      "Parsing 'MV4__3' chart with 148 functions\n",
      "Parsing 'MV4__4' chart with 148 functions\n",
      "Parsing 'MV4__5' chart with 148 functions\n",
      "Parsing 'MV5__1' chart with 265 functions\n",
      "Parsing 'MV5__2' chart with 265 functions\n",
      "Parsing 'MV5__3' chart with 265 functions\n",
      "Parsing 'MV5__4' chart with 265 functions\n",
      "Parsing 'MV5__5' chart with 265 functions\n",
      "Parsing 'MV6__1' chart with 122 functions\n",
      "Parsing 'MV6__2' chart with 122 functions\n",
      "Parsing 'MV6__3' chart with 122 functions\n",
      "Parsing 'GF___3' chart with 317 functions\n",
      "Parsing 'GF___4' chart with 317 functions\n",
      "Parsing 'GF___5' chart with 318 functions\n",
      "Parsing 'INF__1' chart with 57 functions\n",
      "Parsing 'INF__2' chart with 53 functions\n",
      "Parsing 'INF__3' chart with 53 functions\n",
      "Parsing 'INF__4' chart with 53 functions\n",
      "Parsing 'INF__5' chart with 53 functions\n",
      "Parsing 'INHWN1' chart with 285 functions\n",
      "Parsing cancelled: 'INHWN1' chart is not used in SimadynD program\n",
      "Parsing 'INHWO1' chart with 285 functions\n",
      "Parsing cancelled: 'INHWO1' chart is not used in SimadynD program\n",
      "Parsing 'INHW_1' chart with 372 functions\n",
      "Parsing 'INHW_2' chart with 169 functions\n",
      "Parsing 'INHW_3' chart with 169 functions\n",
      "Parsing 'INHW_4' chart with 168 functions\n",
      "Parsing 'INHW_5' chart with 172 functions\n",
      "Parsing 'INSW_1' chart with 97 functions\n",
      "Parsing 'INSW_2' chart with 71 functions\n",
      "Parsing 'INSW_3' chart with 71 functions\n",
      "Parsing 'INSW_4' chart with 71 functions\n",
      "Parsing 'INSW_5' chart with 71 functions\n",
      "Parsing 'MESY_1' chart with 172 functions\n",
      "Parsing 'MESY_2' chart with 163 functions\n",
      "Parsing 'MESY_3' chart with 162 functions\n",
      "Parsing 'MESY_4' chart with 162 functions\n",
      "Parsing 'MESY_5' chart with 163 functions\n",
      "Parsing 'MMID_1' chart with 33 functions\n",
      "Parsing 'MMID_2' chart with 31 functions\n",
      "Parsing 'MMID_3' chart with 31 functions\n",
      "Parsing 'MMID_4' chart with 31 functions\n",
      "Parsing 'MMID_5' chart with 31 functions\n",
      "Parsing 'MMII_1' chart with 76 functions\n",
      "Parsing 'CTRC_1' chart with 38 functions\n",
      "Parsing 'CTRC_2' chart with 24 functions\n",
      "Parsing 'CTRC_3' chart with 24 functions\n",
      "Parsing 'CTRC_4' chart with 24 functions\n",
      "Parsing 'CTRC_5' chart with 24 functions\n",
      "Parsing 'CTRS_1' chart with 21 functions\n",
      "Parsing 'CTRS_2' chart with 13 functions\n",
      "Parsing 'CTRS_3' chart with 13 functions\n",
      "Parsing 'CTRS_4' chart with 13 functions\n",
      "Parsing 'CTRS_5' chart with 13 functions\n",
      "Parsing 'GF___1' chart with 317 functions\n",
      "Parsing 'GF___2' chart with 317 functions\n",
      "Parsing '@SND_1' chart with 10 functions\n",
      "Parsing '@SND_2' chart with 3 functions\n",
      "Parsing '@SND_3' chart with 3 functions\n",
      "Parsing '@SND_4' chart with 3 functions\n",
      "Parsing '@SND_5' chart with 3 functions\n",
      "Parsing 'CHC__1' chart with 136 functions\n",
      "Parsing 'CHC__2' chart with 136 functions\n",
      "Parsing 'CHC__3' chart with 136 functions\n",
      "Parsing 'CHC__4' chart with 136 functions\n",
      "Parsing 'CHC__5' chart with 135 functions\n",
      "Parsing cpu file '1sx04'  \n",
      "PN-D01_P1\n",
      "Parsing cpu file '2sx04'  \n",
      "PN-D04_P2\n",
      "Parsing cpu file '3sx04'  \n",
      "PN-D06_P3\n",
      "Parsing cpu file '4sx04'  \n",
      "PN-D08_P4\n",
      "Parsing cpu file '5sx04'  \n",
      "PN-D10_P5\n",
      "Parsing 'WPS__5' chart with 7 functions\n",
      "Parsing cancelled: 'WPS__5' chart is not used in SimadynD program\n",
      "Parsing 'PARA_5' chart with 9 functions\n",
      "Parsing 'RACK_1' chart with 21 functions\n",
      "Parsing 'RCEV_1' chart with 34 functions\n",
      "Parsing 'RCEV_2' chart with 30 functions\n",
      "Parsing 'RCEV_3' chart with 30 functions\n",
      "Parsing 'RCEV_4' chart with 30 functions\n",
      "Parsing 'RCEV_5' chart with 30 functions\n",
      "Parsing 'REF__1' chart with 172 functions\n",
      "Parsing 'REF__2' chart with 172 functions\n",
      "Parsing 'REF__3' chart with 172 functions\n",
      "Parsing 'REF__4' chart with 172 functions\n",
      "Parsing 'REF__5' chart with 172 functions\n",
      "Parsing 'SEQ__1' chart with 185 functions\n",
      "Parsing 'SEQ__2' chart with 185 functions\n",
      "Parsing 'SEQ__3' chart with 185 functions\n",
      "Parsing 'SEQ__4' chart with 185 functions\n",
      "Parsing 'SEQ__5' chart with 185 functions\n",
      "Parsing 'SERV_1' chart with 12 functions\n",
      "Parsing 'SERV_2' chart with 12 functions\n",
      "Parsing 'SERV_3' chart with 12 functions\n",
      "Parsing 'SERV_4' chart with 12 functions\n",
      "Parsing 'SERV_5' chart with 12 functions\n",
      "Parsing 'TWAS_1' chart with 27 functions\n",
      "Parsing cancelled: 'TWAS_1' chart is not used in SimadynD program\n",
      "Parsing 'WBER_1' chart with 130 functions\n",
      "Parsing 'WBER_2' chart with 130 functions\n",
      "Parsing 'WBER_3' chart with 130 functions\n",
      "Parsing 'WBER_4' chart with 130 functions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing 'WBER_5' chart with 130 functions\n",
      "Parsing 'WPS__1' chart with 7 functions\n",
      "Parsing cancelled: 'WPS__1' chart is not used in SimadynD program\n",
      "Parsing 'WPS__2' chart with 7 functions\n",
      "Parsing cancelled: 'WPS__2' chart is not used in SimadynD program\n",
      "Parsing 'WPS__3' chart with 7 functions\n",
      "Parsing cancelled: 'WPS__3' chart is not used in SimadynD program\n",
      "Parsing 'WPS__4' chart with 7 functions\n",
      "Parsing cancelled: 'WPS__4' chart is not used in SimadynD program\n",
      "Parsing 'MV6__2' chart with 122 functions\n",
      "Parsing 'MV6__3' chart with 122 functions\n",
      "Parsing 'MV6__4' chart with 122 functions\n",
      "Parsing 'MV6__5' chart with 122 functions\n",
      "Parsing 'MV7__1' chart with 261 functions\n",
      "Parsing 'MV7__2' chart with 260 functions\n",
      "Parsing 'MV7__3' chart with 260 functions\n",
      "Parsing 'MV7__4' chart with 260 functions\n",
      "Parsing 'MV7__5' chart with 260 functions\n",
      "Parsing 'MV8__1' chart with 110 functions\n",
      "Parsing 'OPD__1' chart with 95 functions\n",
      "Parsing 'OPD__2' chart with 95 functions\n",
      "Parsing 'OPD__3' chart with 95 functions\n",
      "Parsing 'OPD__4' chart with 95 functions\n",
      "Parsing 'OPD__5' chart with 95 functions\n",
      "Parsing 'OUTHN2' chart with 56 functions\n",
      "Parsing cancelled: 'OUTHN2' chart is not used in SimadynD program\n",
      "Parsing 'OUTHW1' chart with 71 functions\n",
      "Parsing 'OUTHW2' chart with 60 functions\n",
      "Parsing 'OUTHW3' chart with 58 functions\n",
      "Parsing 'OUTHW4' chart with 60 functions\n",
      "Parsing 'OUTHW5' chart with 56 functions\n",
      "Parsing 'OUTSW1' chart with 94 functions\n",
      "Parsing 'OUTSW2' chart with 70 functions\n",
      "Parsing 'OUTSW3' chart with 64 functions\n",
      "Parsing 'OUTSW4' chart with 64 functions\n",
      "Parsing 'OUTSW5' chart with 67 functions\n",
      "Parsing 'PARA_1' chart with 9 functions\n",
      "Parsing 'PARA_2' chart with 9 functions\n",
      "Parsing 'PARA_3' chart with 9 functions\n",
      "Parsing 'PARA_4' chart with 9 functions\n",
      "Parsing 'MMII_1' chart with 76 functions\n",
      "Parsing 'MMII_2' chart with 76 functions\n",
      "Parsing 'MMII_3' chart with 76 functions\n",
      "Parsing 'MMII_4' chart with 76 functions\n",
      "Parsing 'MMII_5' chart with 76 functions\n",
      "Parsing 'MV1__1' chart with 268 functions\n",
      "Parsing 'MV1__2' chart with 259 functions\n",
      "Parsing 'MV1__3' chart with 259 functions\n",
      "Parsing 'MV1__4' chart with 259 functions\n",
      "Parsing 'MV1__5' chart with 259 functions\n",
      "Parsing 'MV2__1' chart with 151 functions\n",
      "Parsing 'MV2__2' chart with 151 functions\n",
      "Parsing 'MV2__3' chart with 151 functions\n",
      "Parsing 'MV2__4' chart with 151 functions\n",
      "Parsing 'MV2__5' chart with 204 functions\n",
      "Parsing 'MV3__1' chart with 132 functions\n",
      "Parsing 'MV3__2' chart with 132 functions\n",
      "Parsing 'MV3__3' chart with 132 functions\n",
      "Parsing 'MV3__4' chart with 132 functions\n",
      "Parsing 'MV3__5' chart with 132 functions\n",
      "Parsing 'MV4__1' chart with 148 functions\n",
      "Parsing 'MV4__2' chart with 148 functions\n",
      "Parsing 'MV4__3' chart with 148 functions\n",
      "Parsing 'MV4__4' chart with 148 functions\n",
      "Parsing 'MV4__5' chart with 148 functions\n",
      "Parsing 'MV5__1' chart with 265 functions\n",
      "Parsing 'MV5__2' chart with 265 functions\n",
      "Parsing 'MV5__3' chart with 265 functions\n",
      "Parsing 'MV5__4' chart with 265 functions\n",
      "Parsing 'MV5__5' chart with 265 functions\n",
      "Parsing 'MV6__1' chart with 122 functions\n",
      "Parsing 'GF___3' chart with 317 functions\n",
      "Parsing 'GF___4' chart with 317 functions\n",
      "Parsing 'GF___5' chart with 318 functions\n",
      "Parsing 'INF__1' chart with 57 functions\n",
      "Parsing 'INF__2' chart with 53 functions\n",
      "Parsing 'INF__3' chart with 53 functions\n",
      "Parsing 'INF__4' chart with 53 functions\n",
      "Parsing 'INF__5' chart with 53 functions\n",
      "Parsing 'INHWN1' chart with 285 functions\n",
      "Parsing cancelled: 'INHWN1' chart is not used in SimadynD program\n",
      "Parsing 'INHWN2' chart with 217 functions\n",
      "Parsing cancelled: 'INHWN2' chart is not used in SimadynD program\n",
      "Parsing 'INHWO1' chart with 285 functions\n",
      "Parsing cancelled: 'INHWO1' chart is not used in SimadynD program\n",
      "Parsing 'INHW_1' chart with 372 functions\n",
      "Parsing 'INHW_2' chart with 217 functions\n",
      "Parsing 'INHW_3' chart with 169 functions\n",
      "Parsing 'INHW_4' chart with 168 functions\n",
      "Parsing 'INHW_5' chart with 172 functions\n",
      "Parsing 'INSW_1' chart with 97 functions\n",
      "Parsing 'INSW_2' chart with 71 functions\n",
      "Parsing 'INSW_3' chart with 71 functions\n",
      "Parsing 'INSW_4' chart with 71 functions\n",
      "Parsing 'INSW_5' chart with 71 functions\n",
      "Parsing 'MESY_1' chart with 172 functions\n",
      "Parsing 'MESY_2' chart with 163 functions\n",
      "Parsing 'MESY_3' chart with 162 functions\n",
      "Parsing 'MESY_4' chart with 162 functions\n",
      "Parsing 'MESY_5' chart with 163 functions\n",
      "Parsing 'MMID_1' chart with 33 functions\n",
      "Parsing 'MMID_2' chart with 31 functions\n",
      "Parsing 'MMID_3' chart with 31 functions\n",
      "Parsing 'MMID_4' chart with 31 functions\n",
      "Parsing 'MMID_5' chart with 31 functions\n",
      "Parsing 'CTRC_1' chart with 38 functions\n",
      "Parsing 'CTRC_2' chart with 24 functions\n",
      "Parsing 'CTRC_3' chart with 24 functions\n",
      "Parsing 'CTRC_4' chart with 24 functions\n",
      "Parsing 'CTRC_5' chart with 24 functions\n",
      "Parsing 'CTRS_1' chart with 21 functions\n",
      "Parsing 'CTRS_2' chart with 13 functions\n",
      "Parsing 'CTRS_3' chart with 13 functions\n",
      "Parsing 'CTRS_4' chart with 13 functions\n",
      "Parsing 'CTRS_5' chart with 13 functions\n",
      "Parsing 'GF___1' chart with 317 functions\n",
      "Parsing 'GF___2' chart with 317 functions\n",
      "Parsing '@SND_1' chart with 10 functions\n",
      "Parsing '@SND_2' chart with 3 functions\n",
      "Parsing '@SND_3' chart with 3 functions\n",
      "Parsing '@SND_4' chart with 3 functions\n",
      "Parsing '@SND_5' chart with 3 functions\n",
      "Parsing 'CHC__1' chart with 136 functions\n",
      "Parsing 'CHC__2' chart with 136 functions\n",
      "Parsing 'CHC__3' chart with 136 functions\n",
      "Parsing 'CHC__4' chart with 136 functions\n",
      "Parsing 'CHC__5' chart with 135 functions\n"
     ]
    }
   ],
   "source": [
    "###########EXECUTION CELL###########\n",
    "if(COMPARISON_TYPE==COMTYPE_SDSD):\n",
    "    sd1_data_lm = sd_build_cpu_map(sd1_cpu_charts_list)\n",
    "    sd1_data_s1,sd1_data_ls1 = sd_build_data_s1(sd1_chart_list,sd1_data_lm)\n",
    "    sd2_data_lm = sd_build_cpu_map(sd2_cpu_charts_list)\n",
    "    sd2_data_s1,sd2_data_ls1 = sd_build_data_s1(sd2_chart_list,sd2_data_lm)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_SDTDC):  \n",
    "    sd1_data_lm = sd_build_cpu_map(sd1_cpu_charts_list)\n",
    "    sd1_data_s1,sd1_data_ls1 = sd_build_data_s1(sd1_chart_list,sd1_data_lm)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCSD):  \n",
    "    raise Exception('TDC to SD not implemented yet')\n",
    "    sd2_data_lm = sd_build_cpu_map(sd2_cpu_charts_list)\n",
    "    sd2_data_s1,sd2_data_ls1 = sd_build_data_s1(sd2_chart_list,sd2_data_lm)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCTDC):  \n",
    "    raise Exception('TDC to TDC not implemented yet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########EXECUTION CELL###########\n",
    "#save intermidiate result\n",
    "if(COMPARISON_TYPE==COMTYPE_SDSD):\n",
    "    sd1_data_s1.to_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP1))\n",
    "    sd1_data_ls1.to_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP2))\n",
    "    sd1_data_lm.to_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP3))\n",
    "    sd2_data_s1.to_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP1))\n",
    "    sd2_data_ls1.to_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP2))\n",
    "    sd2_data_lm.to_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP3))\n",
    "elif  (COMPARISON_TYPE==COMTYPE_SDTDC):  \n",
    "    sd1_data_s1.to_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP1))\n",
    "    sd1_data_ls1.to_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP2))\n",
    "    sd1_data_lm.to_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP3))\n",
    "    tdc2_data_a1.to_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP1))\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCSD):  \n",
    "    raise Exception('TDC to SD not implemented yet')\n",
    "    tdc1_data_a1.to_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP1))\n",
    "    sd2_data_s1.to_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP1))\n",
    "    sd2_data_ls1.to_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP2))\n",
    "    sd2_data_lm.to_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP3))\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCTDC):  \n",
    "    raise Exception('TDC to TDC not implemented yet')\n",
    "    tdc1_data_a1.to_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP1))\n",
    "    tdc2_data_a1.to_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########EXECUTION CELL###########\n",
    "#restore intermediate result\n",
    "if(COMPARISON_TYPE==COMTYPE_SDSD):\n",
    "    sd1_data_s1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP1),index_col=0)\n",
    "    sd1_data_ls1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP2),index_col=0)\n",
    "    sd1_data_lm = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP3),index_col=0)\n",
    "    sd2_data_s1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP1),index_col=0)\n",
    "    sd2_data_ls1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP2),index_col=0)\n",
    "    sd2_data_lm = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP3),index_col=0)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_SDTDC):  \n",
    "    sd1_data_s1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP1),index_col=0)\n",
    "    sd1_data_ls1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP2),index_col=0)\n",
    "    sd1_data_lm = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP3),index_col=0)\n",
    "    tdc2_data_a1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP1),index_col=0)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCSD):  \n",
    "    raise Exception('TDC to SD not implemented yet')\n",
    "    tdc1_data_a1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP1),index_col=0)\n",
    "    sd2_data_s1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP1),index_col=0)\n",
    "    sd2_data_ls1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP2),index_col=0)\n",
    "    sd2_data_lm = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP3),index_col=0)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCTDC):  \n",
    "    raise Exception('TDC to TDC not implemented yet')\n",
    "    tdc1_data_a1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_1_NAME_TEMP1),index_col=0)\n",
    "    tdc2_data_a1 = pd.read_csv(os.path.join(PATH_TO_DATA,FILE_2_NAME_TEMP1),index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########EXECUTION CELL###########\n",
    "#find links for sd\n",
    "if(COMPARISON_TYPE==COMTYPE_SDSD):\n",
    "    sd1_data_s2 = sd_rect_data_s1(sd1_data_s1.copy(),sd1_data_ls1)\n",
    "    sd1_data_s3 = sd_rect_data_s2(sd1_data_s2.copy())\n",
    "    sd2_data_s2 = sd_rect_data_s1(sd2_data_s1.copy(),sd2_data_ls1)\n",
    "    sd2_data_s3 = sd_rect_data_s2(sd2_data_s2.copy())\n",
    "elif  (COMPARISON_TYPE==COMTYPE_SDTDC):  \n",
    "    sd1_data_s2 = sd_rect_data_s1(sd1_data_s1.copy(),sd1_data_ls1)\n",
    "    sd1_data_s3 = sd_rect_data_s2(sd1_data_s2.copy())\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCSD):  \n",
    "    raise Exception('TDC to SD not implemented yet')\n",
    "    sd2_data_s2 = sd_rect_data_s1(sd2_data_s1.copy(),sd2_data_ls1)\n",
    "    sd2_data_s3 = sd_rect_data_s2(sd2_data_s2.copy())\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCTDC):  \n",
    "    raise Exception('TDC to TDC not implemented yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########LIBRARY CELL###########\n",
    "#report generator of comparison of block in all SD1 and SD2 charts\n",
    "def report_find_miss_blocks_sdsd (sd1_data_sd,sd2_data_sd, to_file = True, timestamp = datetime.datetime.now()):\n",
    "    README_HEADER = COMPARATOR_README +\"\\\n",
    "\\r\\nReport: find missing blocks in sources\\\n",
    "\\r\\nDate ant time of report generation: %s \\\n",
    "\\r\\n===================================================================================\\\n",
    "\\r\\n\"%(str(timestamp))\n",
    "    \n",
    "    def generate_report(f):\n",
    "        print(README_HEADER,file = f) \n",
    "        sd1_chartblock_sel = sd1_data_sd[\"chart\"]+\"/\"+sd1_data_sd[\"block\"]\n",
    "        sd1_uniq_blocks = sd1_chartblock_sel.unique()\n",
    "        #tdc_uniq_blocks = data_tdc['chartblock'].unique()\n",
    "        sd2_chartblock_sel = sd2_data_sd[\"chart\"]+\"/\"+sd2_data_sd[\"block\"]\n",
    "        sd2_uniq_blocks = sd2_chartblock_sel.unique()\n",
    "        #look for A in B\n",
    "        for i in sd1_uniq_blocks:\n",
    "            if not i in sd2_uniq_blocks:\n",
    "                print(\"Missing block in SD#2: \",i,\"\\r\\n\",file = f) \n",
    "        #look for B in A\n",
    "        for i in sd2_uniq_blocks:\n",
    "            if not i in sd1_uniq_blocks:\n",
    "                print(\"Missing block in SD#1: \",i,\"\\r\\n\",file = f) \n",
    "  \n",
    "    if to_file:\n",
    "        file_suffix = \"@_compare_blocks_sdsd_%s\"%(str(timestamp.date()))\n",
    "        file_prefix = \"report_@\"\n",
    "        try:\n",
    "            # Create target Directory\n",
    "            os.mkdir(os.path.join(PATH_TO_REPORTS))\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        with open(os.path.join(PATH_TO_REPORTS,(file_prefix+SD_RACK_NAMES[0]+file_suffix+\".txt\")), 'w') as f: \n",
    "            generate_report(f)\n",
    "    else:\n",
    "        generate_report(sys.stdout)\n",
    "        \n",
    "#report generator of comparison of block in all SD and TDC charts\n",
    "def report_find_miss_blocks_sdtdc (data_sd,data_tdc, to_file = True, timestamp = datetime.datetime.now()):\n",
    "    README_HEADER = COMPARATOR_README +\"\\\n",
    "\\r\\nReport: find missing blocks in sources\\\n",
    "\\r\\nDate ant time of report generation: %s \\\n",
    "\\r\\n===================================================================================\\\n",
    "\\r\\n\"%(str(timestamp))\n",
    "    \n",
    "    def generate_report(f):\n",
    "        print(README_HEADER,file = f) \n",
    "        chartblock_sel = data_sd[\"chart\"]+\"/\"+data_sd[\"block\"]\n",
    "        sd_uniq_blocks = chartblock_sel.unique()\n",
    "        tdc_uniq_blocks = data_tdc['chartblock'].unique()\n",
    "        for i in sd_uniq_blocks:\n",
    "            if not i in tdc_uniq_blocks:\n",
    "                print(\"Missing block in TDC: \",i,\"\\r\\n\",file = f) \n",
    "                \n",
    "        #look for A in B\n",
    "        for i in sd_uniq_blocks:\n",
    "            if not i in tdc_uniq_blocks:\n",
    "                print(\"Missing block in TDC: \",i,\"\\r\\n\",file = f) \n",
    "        #look for B in A\n",
    "        for i in tdc_uniq_blocks :\n",
    "            if not i in sd_uniq_blocks:\n",
    "                print(\"Missing block in SD: \",i,\"\\r\\n\",file = f)   \n",
    "                \n",
    "    if to_file:\n",
    "        file_suffix = \"@_compare_blocks_sdtdc_%s\"%(str(timestamp.date()))\n",
    "        file_prefix = \"report_@\"\n",
    "        try:\n",
    "            # Create target Directory\n",
    "            os.mkdir(os.path.join(PATH_TO_REPORTS))\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        with open(os.path.join(PATH_TO_REPORTS,(file_prefix+SD_RACK_NAMES[0]+file_suffix+\".txt\")), 'w') as f: \n",
    "            generate_report(f)\n",
    "    else:\n",
    "        generate_report(sys.stdout)\n",
    "        \n",
    "###########LIBRARY CELL###########\n",
    "#####report of comparison of all SD blocks inputs, including links\n",
    "\n",
    "        #['BOOL' 'INT' 'WORD' 'SDTIME' 'GLOBAL' 'STRING' 'REAL' 'DINT' 'BYTE' 'DWORD']\n",
    "        #SD types:\n",
    "        #'B1' - BOOL (0/1/link)\n",
    "        #'TF' - Time (ffff.fff[s/min/S/ms])\n",
    "        #'N2' - Integer (ddddd)\n",
    "        #'V2' - Integer HEX\n",
    "        #'MR' - Message link (e.g MESY_1)(Inout = \"-\")\n",
    "        #'NS' - String (Inout = \"-\")\n",
    "        #'O2' - Integer (dddddd)\n",
    "        #'CR' - Hardware connectors, string\n",
    "        #'V4' - Dword HEX\n",
    "        #'NF' - Floating point\n",
    "        #'V1' - Byte HEX   \n",
    "        #'I2' - Integer\n",
    "        #'NK' - Hardware connectors, string (Inout = \"=\")\n",
    "        #'TR' - Telegram transmit connector e.g. !EXM504 (Inout = \"=\")\n",
    "        #'N4' - Integer 32 bits (%/HEX/ddd)\n",
    "        #'RR' - Telegram recieve connector e.g. !T4EX05 (Inout = \"=\") \n",
    "        \n",
    "\n",
    "\n",
    "def report_comp_signals_sdsd (s1_inp_data,s2_inp_data, to_file = True, timestamp = datetime.datetime.now()):\n",
    "    README_HEADER = COMPARATOR_README +\"\\\n",
    "\\r\\nReport: compare all SD#1 inputs (including links) with SD#2\\\n",
    "\\r\\nDate ant time of report generation: %s \\\n",
    "\\r\\n===================================================================================\\\n",
    "\\r\\n\"%(str(timestamp))\n",
    "    sd1_data_sd = s1_inp_data.copy()\n",
    "    sd2_data_sd = s2_inp_data.copy()\n",
    "    \n",
    "    def clean_sd_tdc_str_values(s1_value,s2_value,sd_type,sd_type_conv,chart_block_signal,source_dest_text,f):\n",
    "        #check if values are empty\n",
    "        if  pd.isnull(s1_value):\n",
    "            if pd.isnull(s2_value):\n",
    "                return True\n",
    "            else:\n",
    "                result = False;\n",
    "        else:\n",
    "            if pd.isnull(s2_value):\n",
    "                result = False;\n",
    "            else:\n",
    "                result = s1_value == s2_value;\n",
    "            \n",
    "        if not result:\n",
    "            print(\"\\r\\nDifference found in \",source_dest_text,chart_block_signal,\": \",s1_value,'==>>> ',s2_value,file = f)\n",
    "        return result\n",
    "\n",
    "    def generate_report(f):\n",
    "        print(README_HEADER,file = f) \n",
    "        \n",
    "        #filter sd data frame\n",
    "        sd1_data_sd_filtered = sd1_data_sd[(sd1_data_sd['type_base'].isin(filt_type_base))&(sd1_data_sd['inout'].isin(filt_inout))]\n",
    "        sd2_data_sd_filtered = sd2_data_sd[(sd2_data_sd['type_base'].isin(filt_type_base))&(sd2_data_sd['inout'].isin(filt_inout))]\n",
    "        \n",
    "        sd1_data_sd_filtered['chart_block_signal'] = sd1_data_sd_filtered[\"chart\"]+\"/\"+sd1_data_sd_filtered[\"block\"]+\".\"+sd1_data_sd_filtered[\"signal\"]\n",
    "        sd2_data_sd_filtered['chart_block_signal'] = sd2_data_sd_filtered[\"chart\"]+\"/\"+sd2_data_sd_filtered[\"block\"]+\".\"+sd2_data_sd_filtered[\"signal\"]\n",
    "        \n",
    "        #connect links $XXXXX->$XXXXX\n",
    "        d1 = dict(zip(sd2_data_sd_filtered['chart_block_signal'].values, sd2_data_sd_filtered['value_linked'].values))\n",
    "        d2 = dict(zip(sd1_data_sd_filtered['chart_block_signal'].values, sd1_data_sd_filtered['value_linked'].values))\n",
    "        \n",
    "        sd1_data_sd_filtered['val_to_compare'] = sd1_data_sd_filtered['chart_block_signal'].map(d1)\n",
    "        sd1_data_sd_filtered['compare_val_sd'] = sd1_data_sd_filtered.apply(lambda row: clean_sd_tdc_str_values(\\\n",
    "           row['value_linked'], row['val_to_compare'],row['type_base'], row['type_conv'],row['chart_block_signal'],\"SD#1->SD#2 value:\",f), axis=1)\n",
    "        \n",
    "        sd2_data_sd_filtered['val_to_compare'] = sd2_data_sd_filtered['chart_block_signal'].map(d2)\n",
    "        sd2_data_sd_filtered['compare_val_sd'] = sd2_data_sd_filtered.apply(lambda row: clean_sd_tdc_str_values(\\\n",
    "           row['value_linked'], row['val_to_compare'],row['type_base'], row['type_conv'],row['chart_block_signal'],\"SD#2->SD#1 value:\",f), axis=1)\n",
    "        \n",
    "    if to_file:\n",
    "        file_suffix = \"@_compare_singals_sdsd_%s\"%(str(timestamp.date()))\n",
    "        file_prefix = \"report_@\"\n",
    "        try:\n",
    "            # Create target Directory\n",
    "            os.mkdir(os.path.join(PATH_TO_REPORTS))\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        with open(os.path.join(PATH_TO_REPORTS,(file_prefix+SD_RACK_NAMES[0]+file_suffix+\".txt\")), 'w') as f: \n",
    "            generate_report(f)\n",
    "    else:\n",
    "        generate_report(sys.stdout)\n",
    "        \n",
    "def report_comp_signals_sdtdc (inp_data_sd,inp_data_tdc, to_file = True, timestamp = datetime.datetime.now()):\n",
    "    README_HEADER = COMPARATOR_README +\"\\\n",
    "\\r\\nReport: compare SD signals (including links) with TDC \\\n",
    "\\r\\nDate ant time of report generation: %s \\\n",
    "\\r\\n===================================================================================\\\n",
    "\\r\\n\"%(str(timestamp))\n",
    "    data_sd = inp_data_sd.copy()\n",
    "    data_tdc = inp_data_tdc.copy()\n",
    "    def merge_tdc_link_value(l_value,l_link):\n",
    "        if pd.isnan(l_link):\n",
    "            return l_link\n",
    "        else:\n",
    "            return l_value\n",
    "    \n",
    "    def clean_sd_tdc_str_values(s_value,a_value,sd_type,sd_type_conv,chart_block_signal,inout,f):\n",
    "        #modify HEX function\n",
    "        def twos_complement(hexstr,bits,base=16):\n",
    "            value = int(hexstr,base)\n",
    "            if value & (1 << (bits-1)):\n",
    "                value -= 1 << bits\n",
    "            return value\n",
    " \n",
    "        #for debugging\n",
    "        #print(\"Working on:\",s_value,a_value,sd_type,sd_type_conv)\n",
    "        \n",
    "        #check if values are empty\n",
    "        if  pd.isnull(s_value):\n",
    "            if pd.isnull(a_value):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            if pd.isnull(a_value):\n",
    "                return False\n",
    "        #regex for chart and link $ \n",
    "        con_check = re.compile(r'^[a-zA-Z0-9_\\-@]+\\\\[a-zA-Z0-9_\\-@]+\\.[a-zA-Z0-9_\\-@]+$')\n",
    "        link_check = re.compile(r'^\\$')\n",
    "        link_check2 = re.compile(r'^\\!')\n",
    "        #check if SD is link\n",
    "        #print(\"s_value:\",s_value)\n",
    "        #print(\"a_value:\",a_value)\n",
    "        if con_check.search(s_value) or link_check.search(s_value) or link_check2.search(s_value) \\\n",
    "        or (\"'\" in s_value):\n",
    "            #print(\"Signal value of simadyn has chart connection\")\n",
    "            a_value_res=a_value\n",
    "            s_value_res=s_value\n",
    "        else:\n",
    "            if con_check.search(a_value) or link_check.search(a_value) or link_check2.search(a_value):\n",
    "                #SD is value but TDC is a link\n",
    "                a_value_res=a_value\n",
    "                s_value_res=s_value\n",
    "            else:\n",
    "                #SD and TDC are normal values\n",
    "                if inout == '>':\n",
    "                    #if its output and not link so we don't care\n",
    "                    a_value_res,s_value_res=0,0 #any value\n",
    "                elif sd_type=='NF':\n",
    "                    a_value_res = float(a_value.replace(',','.'))\n",
    "                    if '%' in s_value.split(\"[\")[0]:\n",
    "                        s_value_res = float(re.sub('[^0-9.E-]','', s_value.split(\"[\")[0]))*0.01\n",
    "                    else:\n",
    "                        if sd_type_conv == 'TF':\n",
    "                            units = 1000\n",
    "                            un_txt = s_value.lower()\n",
    "                            un_txt = un_txt.split(\"[\")[1]\n",
    "                            un_txt = un_txt.split(\"]\")[0]\n",
    "                            if un_txt==\"s\":\n",
    "                                units = 1000\n",
    "                            elif un_txt==\"min\":\n",
    "                                units = 60000\n",
    "                            elif un_txt==\"ms\":\n",
    "                                units = 1\n",
    "                            tmp_txt = re.sub('[^0-9.E-]','', s_value)\n",
    "                            s_value_res = int(math.floor(float(tmp_txt)*units))\n",
    "                        else: \n",
    "                            if \"[\" in s_value:\n",
    "                                s_value_res = float(re.sub('[^0-9.E-]','', s_value.split(\"[\")[0]))\n",
    "                            else:\n",
    "                                s_value_res = float(re.sub('[^0-9.E-]','', s_value))  \n",
    "                elif sd_type=='B1':\n",
    "                    if re.match(r'[10]$', s_value.strip()):\n",
    "                        s_value_res = bool(int(s_value))\n",
    "                    else:\n",
    "                        raise\n",
    "                    a_value_res = bool(int(a_value))\n",
    "                elif sd_type=='TF': \n",
    "                    a_value_res = int(a_value.strip('ms'))\n",
    "                    units = 1000\n",
    "                    un_txt = s_value.lower()\n",
    "                    un_txt = un_txt.split(\"[\")[1]\n",
    "                    un_txt = un_txt.split(\"]\")[0]\n",
    "                    if un_txt==\"s\":\n",
    "                        units = 1000\n",
    "                    elif un_txt==\"min\":\n",
    "                        units = 60000\n",
    "                    elif un_txt==\"ms\":\n",
    "                        units = 1\n",
    "                    tmp_txt = re.sub('[^0-9.E-]','', s_value)\n",
    "                    s_value_res = int(math.floor(float(tmp_txt)*units))\n",
    "\n",
    "                elif sd_type in ['N2','N4','V2','O2','V4','V1','I2']:\n",
    "                    #edit TDC value\n",
    "                    if \"16#\" in a_value:\n",
    "                        tmp_txt = a_value.split('16#')[1]\n",
    "                        a_value_res = twos_complement(tmp_txt.strip(),16)    \n",
    "                    else:\n",
    "                        a_value_res = int(a_value)\n",
    "                    #prepare SD value    \n",
    "                    if \"H\" in s_value:\n",
    "                        tmp_txt = s_value.split('0H')[1]\n",
    "                        s_value_res = twos_complement(tmp_txt.strip(),16)\n",
    "                    elif '%' in s_value:\n",
    "                        s_value_res = int(round(float(re.sub('[^0-9.E-]','', s_value))*16384*0.01))\n",
    "                    elif \"B\" in s_value:\n",
    "                        tmp_txt = s_value.split('0B')[1]\n",
    "                        tmp_txt =  tmp_txt.replace(\" \", \"\")\n",
    "                        s_value_res = twos_complement(tmp_txt.strip(),16,base = 2)\n",
    "                    else:\n",
    "                        try:\n",
    "                            s_value_res = int(re.sub('[^0-9.E-]','', s_value)) \n",
    "                        except ValueError:\n",
    "                            print(\"ValueError: s_value=%s,a_value=%s,sd_type=%s,sd_type_conv=%s,chart_block_signal=%s\"%(s_value,a_value,sd_type,sd_type_conv,chart_block_signal))\n",
    "                            raise\n",
    "                elif sd_type in ['TR','RR','NS','MR','CR']:\n",
    "                    a_value_res = a_value.strip(\"'\")\n",
    "                    s_value_res = s_value.strip(\"'\")\n",
    "                elif sd_type in ['NK']:\n",
    "                    a_value_res = a_value.strip(\"*\")\n",
    "                    s_value_res = s_value.strip(\"*\")\n",
    "                else:\n",
    "                    print(\"\\r\\nFault in:\",s_value,a_value,sd_type,sd_type_conv)\n",
    "                    raise\n",
    "        if not isinstance(s_value_res, str):\n",
    "            if float(a_value_res)!=0 and float(s_value_res)!=0:\n",
    "                result = abs((float(s_value_res)-float(a_value_res))/float(s_value_res))<0.005\n",
    "            else:\n",
    "                result = s_value_res==a_value_res\n",
    "        else:\n",
    "            result = s_value_res==a_value_res\n",
    "        if not result:\n",
    "            print(\"\\r\\nDifference found in: \",chart_block_signal,\"SD/TDC value:\",s_value_res,'==>>> ',a_value_res,file = f)\n",
    "        return result\n",
    "\n",
    "    def generate_report(f):\n",
    "        print(README_HEADER,file = f) \n",
    "        #this comparator works with following data types and connectors:\n",
    "        #filter sd data frame\n",
    "        data_sd_filtered = data_sd[(data_sd['type_base'].isin(filt_type_base))&(data_sd['inout'].isin(filt_inout))]\n",
    "        \n",
    "        #connect links $XXXXX->$XXXXX\n",
    "        d = dict(zip(data_tdc['chart_block_signal'].values, data_tdc['value_linked'].values))\n",
    "        data_sd_filtered['chart_block_signal'] = data_sd_filtered[\"chart\"]+\"/\"+data_sd_filtered[\"block\"]+\".\"+data_sd_filtered[\"signal\"]\n",
    "        \n",
    "        data_sd_filtered['val_tdc'] = data_sd_filtered['chart_block_signal'].map(d)\n",
    "        data_sd_filtered['compare_val_sd'] = data_sd_filtered.apply(lambda row: clean_sd_tdc_str_values(\\\n",
    "           row['value_linked'], row['val_tdc'],row['type_base'], row['type_conv'],row['chart_block_signal'],row['inout'],f), axis=1)\n",
    "      \n",
    "    if to_file:\n",
    "        file_suffix = \"@_compare_singals_sdtdc_%s\"%(str(timestamp.date()))\n",
    "        file_prefix = \"report_@\"\n",
    "        try:\n",
    "            # Create target Directory\n",
    "            os.mkdir(os.path.join(PATH_TO_REPORTS))\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        #sys.stdout = open(os.path.join(PATH_TO_REPORTS,(file_prefix+FILE_NAME_DATA1.split(\".\")[0]+file_suffix+\".txt\")), 'w')\n",
    "        with open(os.path.join(PATH_TO_REPORTS,(file_prefix+SD_RACK_NAMES[0]+file_suffix+\".txt\")), 'w') as f: \n",
    "            generate_report(f)\n",
    "    else:\n",
    "        generate_report(sys.stdout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########EXECUTION CELL###########\n",
    "#report find missing blocks\n",
    "if(COMPARISON_TYPE==COMTYPE_SDSD):\n",
    "    report_find_miss_blocks_sdsd(sd1_data_s2,sd2_data_s2,to_file = True)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_SDTDC):  \n",
    "    report_find_miss_blocks_sdtdc(sd1_data_s2,tdc2_data_a1,to_file = True)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCSD):  \n",
    "    raise Exception('TDC to SD not implemented yet')\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCTDC):  \n",
    "    raise Exception('TDC to TDC not implemented yet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########EXECUTION CELL###########\n",
    "#report find missing signals\n",
    "if(COMPARISON_TYPE==COMTYPE_SDSD):\n",
    "    report_comp_signals_sdsd(sd1_data_s3,sd2_data_s3,to_file = True)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_SDTDC):  \n",
    "    report_comp_signals_sdtdc(sd1_data_s3,tdc2_data_a1,to_file = True)\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCSD):  \n",
    "    raise Exception('TDC to SD not implemented yet')\n",
    "elif  (COMPARISON_TYPE==COMTYPE_TDCTDC):  \n",
    "    raise Exception('TDC to TDC not implemented yet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
